{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1ea7bd",
   "metadata": {},
   "source": [
    "# Data Modeling & Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48d0cd",
   "metadata": {},
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc61198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "patient = 3265\n",
    "max_patients = 3265\n",
    "\n",
    "def load_patient_files(patient, dir_path):\n",
    "    tensors = {\n",
    "        'svs': torch.load(f\"{dir_path}{patient}_svs.pt\"),\n",
    "        'clinical': torch.load(f\"{dir_path}{patient}_svs.pt\"),\n",
    "        'copy_segment': torch.load(f\"{dir_path}{patient}_Copy_Number_Segment.pt\"),\n",
    "        'protein': torch.load(f\"{dir_path}{patient}_Protein_Expression_Quantification.pt\"),\n",
    "        'miRNA': torch.load(f\"{dir_path}{patient}_miRNA_Expression_Quantification.pt\"),\n",
    "        'isoform': torch.load(f\"{dir_path}{patient}_Isoform_Expression_Quantification.pt\"),\n",
    "        'gene': torch.load(f\"{dir_path}{patient}_Gene_Expression_Quantification.pt\"),\n",
    "        'allele': torch.load(f\"{dir_path}{patient}_Allele-specific_Copy_Number_Segment.pt\"),\n",
    "    }\n",
    "    \n",
    "    return tensors\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    'svs': {'size': (3, 2400, 2400), 'file': 'svs'},\n",
    "    'masked_somatic': {'size': (1024, 135), 'file': 'Masked_Somatic_Mutation'},\n",
    "    'gene_copy': {'size': (60623, 6), 'file': 'Gene_Level_Copy_Number'},\n",
    "    'clinical': {'size': (768,), 'file': 'Clinical'},\n",
    "    'segment_copy': {'size': (1024, 5), 'file': 'Copy_Number_Segment'},\n",
    "    'protein': {'size': (530,), 'file': 'Protein_Expression_Quantification'},\n",
    "    'miRNA': {'size': (1881, 2), 'file': 'miRNA_Expression_Quantification'},\n",
    "    'isoform': {'size': (1881, 2), 'file': 'Isoform_Expression_Quantification'},\n",
    "    'allele_copy': {'size': (1024, 6), 'file': 'Allele-specific_Copy_Number_Segment'},\n",
    "    'gene_expr': {'size': (60664, 6), 'file': 'Gene_Expression_Quantification'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500831b",
   "metadata": {},
   "source": [
    "## Z-scoring of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08b64ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dir_path = \"/mnt/Cancer_2/processed_data/\"  # replace with your directory path\n",
    "\n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    tensors = []\n",
    "    for patient in range(5):\n",
    "        tensor = torch.load(f\"{dir_path}{patient}_{v['file']}.pt\").detach()\n",
    "        tensors.append(tensor)\n",
    "\n",
    "    # Stacking tensors along a new dimension\n",
    "    X = torch.stack(tensors, dim=0)\n",
    "    lower_clip = torch.nanquantile(X.reshape([-1, X.size()[-1]]), 0.05, dim=0)\n",
    "    upper_clip = torch.nanquantile(X.reshape((-1, X.size()[-1])), 0.95, dim=0)\n",
    "\n",
    "    clipped_X = torch.clamp(X, lower_clip, upper_clip)\n",
    "\n",
    "    # TODO: Use smarter imputation, albeit a lot of it is handled in the embedding layer\n",
    "    clippd_X = torch.nan_to_num(clipped_X, 0.0)\n",
    "    mean = clipped_X.nanmean(dim=[i for i in range(len(v['size']) - 1)], keepdim=True)\n",
    "    std = torch.tensor(np.nanstd(clipped_X.numpy(), axis=tuple([i for i in range(len(v['size']) - 1)]), keepdims=True))\n",
    "\n",
    "    z_scores_tensor = (torch.nan_to_num(X, 0) - mean) / (std + 1e-7)  # adding a small value to prevent division by zero\n",
    "    data_dict[k]['data'] = z_scores_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e3260",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- gene_expr passes through a GRU to produce a 512 one dimensional embedding\n",
    "- allele_copy passes through a GRU to produce a 512 one dimensional embedding\n",
    "- isoform passes through a FF to produce a 512 one dimensional embedding\n",
    "- miRNA passes through a FF to produce a 512 one dimensional embedding\n",
    "- protein passes through a FF to produce a 512 one dimensional embedding\n",
    "- segment_copy passes through a GRU to produce a 512 one dimensional embedding\n",
    "- clinical passes through a FF to produce a 512 one dimensional embedding\n",
    "- gene_copy passes through a GRU to produce a 512 one dimensional embedding\n",
    "- masked_somatic passes through a GRU to produce a 512 one dimensional embedding\n",
    "- svs passes through a CNN (choose whatever architecture you want) to produce a 512 one dimensional embedding\n",
    "\n",
    "masked_somatic, allele_copy, gene_copy, gene_expr, and segment_copy are combined to produce a 512 one dimensional embedding using a FF.\n",
    "\n",
    "Then,  are used to determine the attention for the combination of isoform, and miRNA using a FF to produce a 512 one dimensional embedding using a FF.\n",
    "\n",
    "The two embeddings are used to produce a 1024 one dimensional embedding using a FF. \n",
    "\n",
    "This embedding is used for attention to produce a 512 dimension embedding using protein.\n",
    "\n",
    "Then the 1024 embedding, the newly produced protein embedding, the svs embedding, and the clinical embedding are all combined using a FF to produce a 512 embedding, which is then used to predict one category (true / false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a293a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "svs\n",
      "masked_somatic\n",
      "gene_copy\n",
      "clinical\n",
      "segment_copy\n",
      "protein\n",
      "miRNA\n",
      "isoform\n",
      "allele_copy\n",
      "gene_expr\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=True)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=True)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=True)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        N = query.shape[0]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.view(N, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, self.heads, self.head_dim)\n",
    "        query = query.view(N, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "\n",
    "        # Calculate energy (similarity) with dot product\n",
    "        energy = torch.einsum(\"nhd,nhd->nh\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask.squeeze() == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Apply softmax to have the sum of attention weights equal to 1\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=1)\n",
    "\n",
    "        # Calculate the new values\n",
    "        out = torch.einsum(\"nh,nhd->nhd\", [attention, values]).reshape(\n",
    "            N, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, size_last):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(size_last, 512)\n",
    "        self.layer2 = nn.Linear(512, 1024)\n",
    "        self.layer3 = nn.Linear(1024, 512)\n",
    "\n",
    "        self.relu = nn.ReLU() # activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size()[0], -1)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CombinedDataModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedDataModel, self).__init__()\n",
    "        self.gru_gene_expr = nn.GRU(6, 512, 2)\n",
    "        self.gru_allele_copy = nn.GRU(6, 512, 2)\n",
    "        self.gru_masked_somatic = nn.GRU(135, 512, 2)\n",
    "        self.gru_segment_copy = nn.GRU(5, 512, 2)\n",
    "        self.gru_gene_copy = nn.GRU(6, 512, 2)\n",
    "        \n",
    "        self.isoform = Net(1881 * 2)\n",
    "        self.miRNA = Net(1881 * 2)\n",
    "        self.protein = Net(530)\n",
    "        self.clinical = Net(768)\n",
    "        \n",
    "        self.transcriptomics_combined = Net(512*2)\n",
    "        self.genomics_combined_ff = Net(512*5)\n",
    "        self.gen_and_transcriptomics = Net(512*2)\n",
    "        self.final_linear = Net(512*4)#Net(512*4)\n",
    "        \n",
    "        self.transcriptomics_combined_attention = SelfAttention(512, 8)\n",
    "        self.protein_attention = SelfAttention(512, 8)\n",
    "        \n",
    "        self.output_layer = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gene_exp, _ = self.gru_gene_expr(torch.nan_to_num(x[\"gene_expr\"][\"data\"], 0.0))\n",
    "        gene_exp = gene_exp[:, -1, :]\n",
    "        \n",
    "        allele_copy, _ = self.gru_allele_copy(torch.nan_to_num(x[\"allele_copy\"][\"data\"], 0.0))\n",
    "        allele_copy = allele_copy[:, -1, :]\n",
    "        \n",
    "        masked_somatic, _ = self.gru_masked_somatic(torch.nan_to_num(x[\"masked_somatic\"][\"data\"], 0.0))\n",
    "        masked_somatic = masked_somatic[:, -1, :]\n",
    "        \n",
    "        segment_copy, _ = self.gru_segment_copy(torch.nan_to_num(x[\"segment_copy\"][\"data\"], 0.0))\n",
    "        segment_copy = segment_copy[:, -1, :]\n",
    "        \n",
    "        gene_copy, _ = self.gru_gene_copy(torch.nan_to_num(x[\"gene_copy\"][\"data\"], 0.0))\n",
    "        gene_copy = gene_copy[:, -1, :]\n",
    "        \n",
    "        isoform = self.isoform(torch.reshape(torch.nan_to_num(x[\"isoform\"][\"data\"], 0.0), (x[\"isoform\"][\"data\"].size()[0], -1)))\n",
    "        miRNA = self.miRNA(torch.reshape(torch.nan_to_num(x[\"miRNA\"][\"data\"], 0.0), (x[\"miRNA\"][\"data\"].size()[0], -1)))\n",
    "        protein = self.protein(torch.reshape(torch.nan_to_num(x[\"protein\"][\"data\"], 0.0), (x[\"protein\"][\"data\"].size()[0], -1)))\n",
    "        clinical = self.clinical(torch.reshape(torch.nan_to_num(x[\"clinical\"][\"data\"], 0.0), (x[\"clinical\"][\"data\"].size()[0], -1)))\n",
    "        \n",
    "        combined_genomics = self.genomics_combined_ff(torch.cat([gene_exp, allele_copy, masked_somatic, allele_copy, segment_copy], 1))\n",
    "        combined_transcriptomics = self.transcriptomics_combined(torch.cat([miRNA, isoform], 1))\n",
    "        \n",
    "        attention_transcriptomics = self.transcriptomics_combined_attention(values=combined_transcriptomics, keys=combined_transcriptomics, query=combined_genomics)\n",
    "        \n",
    "        gen_and_transcriptomics = self.gen_and_transcriptomics(torch.cat([combined_genomics, attention_transcriptomics], 1))\n",
    "        \n",
    "        attention_proteomics = self.protein_attention(values=protein, keys=protein, query=gen_and_transcriptomics)\n",
    "\n",
    "        final_embedding = self.final_linear(torch.cat([attention_proteomics, combined_genomics, combined_transcriptomics, clinical], 1))\n",
    "        out = self.output_layer(final_embedding)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "model = CombinedDataModel()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(1)\n",
    "\n",
    "# Let's move our example data to the device\n",
    "for key in data_dict.keys():\n",
    "    print(key)\n",
    "    data_dict[key]['data'] = data_dict[key]['data'].float().to(device)\n",
    "\n",
    "# Use the model\n",
    "output = model(data_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
