{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0383450c",
   "metadata": {},
   "source": [
    "# Data Integration\n",
    "\n",
    "This notebook aims to show how each data type is integrated into CancerSig.\n",
    "\n",
    "See https://gdc.cancer.gov/about-data/gdc-data-processing/biospecimen-data-standardization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a92396f",
   "metadata": {},
   "source": [
    "## Slide Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6d551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openslide import OpenSlide\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "filename = \"/mnt/Cancer_1/data/TCGA-EM-A3FO-01Z-00-DX1.84929069-A2FF-4588-98AF-77DE5D66F1C9.svs\"\n",
    "\n",
    "# Load the image\n",
    "slide = OpenSlide(filename)\n",
    "\n",
    "# Calculate the level of the image pyramid to read from\n",
    "target_size = (640, 640)\n",
    "width, height = slide.dimensions\n",
    "level = slide.get_best_level_for_downsample(max(width, height) / max(target_size))\n",
    "level -= 1\n",
    "if level < 0:\n",
    "    level = 0\n",
    "    \n",
    "level_width, level_height = slide.level_dimensions[level]\n",
    "\n",
    "# Calculate the resize dimensions\n",
    "target_size = (1000, 1000)\n",
    "resize_width, resize_height = target_size\n",
    "\n",
    "# Read the region at the desired level\n",
    "region = slide.read_region((0, 0), level, (level_width, level_height))\n",
    "\n",
    "# Resize the region to the target size\n",
    "resized_region = region.resize((resize_width, resize_height), Image.BICUBIC)\n",
    "\n",
    "region_rgb = resized_region.convert('RGB')\n",
    "\n",
    "# Display the region\n",
    "region_rgb.show()\n",
    "slide.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# miRNA\n",
    "\n",
    "filename = f\"/mnt/Cancer_1/data/{df_metadata.loc[(df_metadata['data.data_category'] == 'Transcriptome Profiling') &  (df_metadata['data.experimental_strategy'] == 'miRNA-Seq')]['data.file_name'].iloc[120]}\"\n",
    "\n",
    "df = pd.read_csv(filename, sep='\\t')\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Count them\n",
    "miRNA_counts = {}\n",
    "for filename in df_metadata.loc[(df_metadata['data.data_category'] == 'Transcriptome Profiling') &  (df_metadata['data.experimental_strategy'] == 'miRNA-Seq')]['data.file_name']:\n",
    "    df = pd.read_csv(f\"/mnt/Cancer_1/data/{filename}\", sep='\\t')\n",
    "    \n",
    "    for miRNA in df['miRNA_ID']:\n",
    "        if miRNA in miRNA_counts:\n",
    "            miRNA_counts[miRNA] += 1\n",
    "        else:\n",
    "            miRNA_counts[miRNA] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac9f3313",
   "metadata": {},
   "source": [
    "## Metadata Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2f7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "url = \"https://api.gdc.cancer.gov/files\"\n",
    "params = {\n",
    "    \"fields\": \"\"\"file_id,demographic.cause_of_death,index_files.created_datetime,cases.days_to_index,annotations.notes,created_datetime,cases.demographic.created_datetime,cases.demographic.updated_datetime,cases.diagnoses.updated_datetime,cases.diagnoses.created_datetime,updated_datetime,cases.diagnoses.cog_liver_stage,cases.diagnoses.classification_of_tumor,cases.diagnoses.clark_level,cases.diagnoses.child_pugh_classification,cases.diagnoses.cancer_detection_method,cases.diagnoses.burkitt_lymphoma_clinical_variant,cases.diagnoses.best_overall_response,cases.diagnoses.ann_arbor_pathologic_stage,cases.diagnoses.ann_arbor_extranodal_involvement,cases.diagnoses.ann_arbor_clinical_stage,cases.diagnoses.ann_arbor_b_symptoms_described,cases.diagnoses.ann_arbor_b_symptoms,cases.diagnoses.ajcc_staging_system_edition,cases.diagnoses.ajcc_pathologic_t,cases.diagnoses.ajcc_pathologic_stage,cases.diagnoses.ajcc_pathologic_n,cases.diagnoses.ajcc_pathologic_m,cases.diagnoses.ajcc_clinical_t,cases.diagnoses.ajcc_clinical_stage,cases.diagnoses.ajcc_clinical_n,cases.diagnoses.ajcc_clinical_m,cases.diagnoses.adrenal_hormone,cases.diagnoses.tissue_or_organ_of_origin,cases.diagnoses.site_of_resection_or_biopsy,cases.diagnoses.primary_diagnosis,cases.diagnoses.morphology,cases.diagnoses.diagnosis_is_primary_disease,cases.diagnoses.age_at_diagnosis,cases.diagnosis,cases.demographic.race,cases.demographic.vital_status,cases.demographic.age_at_index,cases.demographic.age_is_obfuscated,cases.demographic.cause_of_death,cases.demographic.cause_of_death_source,cases.demographic.country_of_birth,cases.demographic.country_of_residence_at_enrollment,cases.demographic.days_to_birth,cases.demographic.days_to_death,cases.demographic.education_level,cases.demographic.marital_status,cases.demographic.occupation_duration_years,cases.demographic.premature_at_birth,cases.demographic.weeks_gestation_at_birth,cases.demographic.year_of_birth,cases.demographic.year_of_death,cases.demographic.ethnicity,cases.demographic.gender,cases.disease_type,cases.primary_site,cases.index_date,cases.consent_type,cases.days_to_consent,cases.days_to_lost_to_followup,cases.lost_to_followup,file_name,cases.submitter_id,cases.case_id,data_category,data_type,cases.samples.tumor_descriptor,cases.samples.tissue_type,cases.samples.sample_type,cases.samples.submitter_id,cases.samples.sample_id,analysis.workflow_type,cases.project.project_id,cases.samples.portions.analytes.aliquots.aliquot_id,cases.samples.portions.analytes.aliquots.submitter_id\"\"\",\n",
    "    \"from\": 0,\n",
    "    \"size\": 1000\n",
    "}\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "while True:\n",
    "    # make GET request with parameters\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # check that the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # check if there are data in the response\n",
    "        if not data['data']['hits']:\n",
    "            break\n",
    "\n",
    "        # process data\n",
    "        processed_data = []\n",
    "        for hit in data['data']['hits']:\n",
    "            row = hit.copy()\n",
    "\n",
    "            # count the number of cases\n",
    "            row['num_cases'] = len(row['cases'])\n",
    "\n",
    "            # if there are any cases, take the first one\n",
    "            if row['cases']:\n",
    "                case = row['cases'][0].copy()\n",
    "                row.update(case)\n",
    "\n",
    "                # count the number of diagnoses\n",
    "                row['num_diagnoses'] = len(case.get('diagnoses', []))\n",
    "\n",
    "                # if there are any diagnoses, take the first two\n",
    "                for i, diagnosis in enumerate(case.get('diagnoses', [])[:2]):\n",
    "                    row.update({f'diagnosis_{i+1}_{k}': v for k, v in diagnosis.items()})\n",
    "\n",
    "            processed_data.append(row)\n",
    "\n",
    "        # add data to our dataframe\n",
    "        df = pd.json_normalize(processed_data)\n",
    "        all_data = pd.concat([all_data, df])\n",
    "\n",
    "        # update 'from' for the next iteration\n",
    "        params['from'] += params['size']\n",
    "        print(params['from'])\n",
    "    else:\n",
    "        print(\"Request failed with status code\", response.status_code)\n",
    "        break\n",
    "\n",
    "\n",
    "# saving to csv file\n",
    "all_data.to_csv(\"all_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61606ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "filepaths = []\n",
    "selected_ids = []\n",
    "\n",
    "all_data = pd.read_csv('all_data.csv')\n",
    "\n",
    "# This assumes data split between /mnt/Cancer_1 and /mnt/Cancer_2\n",
    "for i, f in enumerate(all_data.file_name):\n",
    "    if os.path.isfile(f\"/mnt/Cancer_1/data/{f}\"):\n",
    "        filepaths.append(f\"/mnt/Cancer_1/data/{f}\")\n",
    "        selected_ids.append(i)\n",
    "    elif os.path.isfile(f\"/mnt/Cancer_2/data/{f}\"):\n",
    "        filepaths.append(f\"/mnt/Cancer_2/data/{f}\")\n",
    "        selected_ids.append(i)\n",
    "\n",
    "processed_metadata = all_data.iloc[selected_ids]\n",
    "processed_metadata['file_path'] = filepaths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6128177a",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6657b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from openslide import OpenSlide\n",
    "import math\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# TODO: Add methylation\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# counter_start_pos = Counter()\n",
    "\n",
    "for filepath in processed_metadata[processed_metadata['data_category'] == 'Simple Nucleotide Variation']['file_path']:\n",
    "    num_comment_lines = sum(1 for line in gzip.open(filepath, 'rt') if line.startswith('#'))\n",
    "    # maf the file, skipping the comment lines\n",
    "    maf = pd.read_csv(filepath, low_memory=False, compression='gzip', sep='\\t', skiprows=num_comment_lines)\n",
    "    for sp in maf.ENSP:\n",
    "        counter_start_pos[sp] += 1\n",
    "\n",
    "top_100_start_positions = counter_start_pos.most_common(100)\n",
    "top_100_start_positions = top_100_start_positions[1:]\n",
    "top_100_ensp = top_100_start_positions\n",
    "\n",
    "def parse_maf(filepath):\n",
    "    num_comment_lines = sum(1 for line in gzip.open(filepath, 'rt') if line.startswith('#'))\n",
    "    # Load the file, skipping the comment lines\n",
    "    maf_df = pd.read_csv(filepath, low_memory=False, compression='gzip', sep='\\t', skiprows=num_comment_lines)\n",
    "    features_maf = ['gnomAD_AF',\n",
    "                'gnomAD_AFR_AF',\n",
    "                'gnomAD_AMR_AF',\n",
    "                'gnomAD_ASJ_AF',\n",
    "                'gnomAD_EAS_AF',\n",
    "                'gnomAD_FIN_AF',\n",
    "                'gnomAD_NFE_AF',\n",
    "                'gnomAD_OTH_AF',\n",
    "                'gnomAD_SAS_AF',\n",
    "                'MAX_AF',\n",
    "                'PolyPhen', # PolyPhen: benign(0.003)\n",
    "                'SIFT',\n",
    "#                 'BIOTYPE',\n",
    "                'One_Consequence',\n",
    "                'Consequence',\n",
    "#                 'Feature_type',\n",
    "#                 'Mutation_Status', All somatic\n",
    "                'Variant_Classification',\n",
    "                'Variant_Type',\n",
    "                'ENSP'\n",
    "               ]\n",
    "    maf_df = maf_df[features_maf]\n",
    "#     maf_df.PolyPhen = maf_df.PolyPhen.str.split('(')[1]\n",
    "    top_100_ensp_ids = [t[0] for t in top_100_ensp]  # Extract the ENSP IDs\n",
    "\n",
    "    # For each ENSP ID in the top 100 list\n",
    "    for ensp in top_100_ensp_ids:\n",
    "#         print( maf_df['ENSP'])\n",
    "        # Add a new column to the maf_df DataFrame\n",
    "        maf_df[f'ENSP_{ensp}'] = maf_df['ENSP'].apply(lambda x: x == ensp)\n",
    "        \n",
    "    for consequence in ['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant', 'intron_variant', 'splice_acceptor_variant', 'splice_donor_variant', 'splice_region_variant', 'non_coding_transcript_exon_variant', 'inframe_deletion', '3_prime_UTR_variant']:\n",
    "        maf_df[f'one_consequence_{consequence}'] = maf_df['One_Consequence'].apply(lambda x: x == ensp)\n",
    "        \n",
    "    for consequence in ['missense_variant', 'synonymous_variant', 'stop_gained', 'frameshift_variant', 'intron_variant', 'splice_acceptor_variant', 'splice_donor_variant', 'splice_region_variant', 'non_coding_transcript_exon_variant', 'inframe_deletion', '3_prime_UTR_variant']:\n",
    "        maf_df[f'consequence_{consequence}'] = maf_df['Consequence'].apply(lambda x: x == ensp)\n",
    "        \n",
    "    for variant_class in ['Missense_Mutation', 'Silent', 'Nonsense_Mutation', 'Frame_Shift_Del', 'Splice_Site']:\n",
    "        maf_df[f'variant_class_{consequence}'] = maf_df['Variant_Classification'].apply(lambda x: x == ensp)\n",
    "    \n",
    "    for variant_type in ['SNP', 'DEL', 'INS']:\n",
    "        maf_df[f'variant_type_{consequence}'] = maf_df['Variant_Type'].apply(lambda x: x == ensp)\n",
    "    \n",
    "    maf_df[\"PolyPhen_score\"] = maf_df[\"PolyPhen\"].astype(str).str.extract(\"\\((.*?)\\)\").astype(float)\n",
    "    maf_df[\"SIFT_score\"] = maf_df[\"SIFT\"].astype(str).str.extract(\"\\((.*?)\\)\").astype(float)\n",
    "    maf_df.drop(columns=['PolyPhen', 'ENSP', 'SIFT', 'Variant_Type', 'Variant_Classification', 'Consequence', 'One_Consequence'], inplace=True)\n",
    "    # Convert the DataFrame to a NumPy array\n",
    "    numpy_array = maf_df.astype(float).values\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    tensor = torch.from_numpy(numpy_array)\n",
    "    tensor = torch.FloatTensor(numpy_array)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# filename = \"/mnt/Cancer_1/data/TCGA-EM-A3FO-01Z-00-DX1.84929069-A2FF-4588-98AF-77DE5D66F1C9.svs\"\n",
    "\n",
    "\n",
    "full_miRNA = pd.read_csv(f\"/mnt/Cancer_1/data/{processed_metadata.loc[(processed_metadata['data_category'] == 'Transcriptome Profiling') & (processed_metadata['data_type'] == 'miRNA Expression Quantification')]['file_name'].iloc[0]}\", delimiter='\\t', skiprows=0)[['miRNA_ID']]\n",
    "\n",
    "# Proteomics Mapping\n",
    "mapping_proteomics = pd.read_csv('mapping_proteomics.tsv', delimiter='\\t').drop_duplicates(subset=['From'])\n",
    "tcga_mapping = pd.read_csv('TCGA_antibodies_descriptions.gencode.v36.tsv', delimiter='\\t').drop_duplicates()\n",
    "tcga_mapping['gene_id'] = tcga_mapping['gene_id'].str.split('/')\n",
    "tcga_mapping = tcga_mapping.explode('gene_id').drop_duplicates()\n",
    "merged_proteomics = tcga_mapping.merge(mapping_proteomics, how='left', left_on='gene_id', right_on='From')\n",
    "\n",
    "\n",
    "# pd.read_csv(f\"/mnt/Cancer_1/data/{df_metadata.loc[(df_metadata['data.data_category'] == 'Transcriptome Profiling') & (df_metadata['data.data_type'] == 'Gene Expression Quantification')]['data.file_name'].iloc[210]}\", delimiter='\\t', skiprows=1)[['unstranded', 'stranded_first', 'stranded_second', 'tpm_unstranded', 'fpkm_unstranded', 'fpkm_uq_unstranded']]\n",
    "def parse_gene_expression_quantification(filepath):\n",
    "    df = pd.read_csv(f\"{filepath}\", delimiter='\\t', skiprows=1)[['unstranded', 'stranded_first', 'stranded_second', 'tpm_unstranded', 'fpkm_unstranded', 'fpkm_uq_unstranded']]\n",
    "    tensor = torch.FloatTensor(df.values)\n",
    "    return tensor\n",
    "\n",
    "# pd.read_csv(f\"/mnt/Cancer_1/data/{df_metadata.loc[(df_metadata['data.data_category'] == 'Transcriptome Profiling') & (df_metadata['data.data_type'] == 'miRNA Expression Quantification')]['data.file_name'].iloc[123]}\", delimiter='\\t', skiprows=0)[['read_count', 'reads_per_million_miRNA_mapped']]\n",
    "def parse_miRNA(filepath):\n",
    "    df = pd.read_csv(f\"{filepath}\", delimiter='\\t', skiprows=0)[['read_count', 'reads_per_million_miRNA_mapped']]\n",
    "    tensor = torch.FloatTensor(df.values)\n",
    "    return tensor\n",
    "\n",
    "# df = pd.read_csv(f\"/mnt/Cancer_1/data/{df_metadata.loc[(df_metadata['data.data_category'] == 'Transcriptome Profiling') & (df_metadata['data.data_type'] == 'Isoform Expression Quantification')]['data.file_name'].iloc[102]}\", delimiter='\\t', skiprows=0)[['miRNA_ID', 'read_count', 'reads_per_million_miRNA_mapped']].groupby('miRNA_ID').sum()\n",
    "def parsed_isoform_to_miRNA(filepath):\n",
    "    df = pd.read_csv(f\"{filepath}\", delimiter='\\t', skiprows=0)[['miRNA_ID', 'read_count', 'reads_per_million_miRNA_mapped']].groupby('miRNA_ID').sum()\n",
    "    full = full_miRNA.merge(df, how='left', on='miRNA_ID')[['read_count', 'reads_per_million_miRNA_mapped']]\n",
    "    tensor = torch.FloatTensor(full.values)\n",
    "    return tensor\n",
    "    \n",
    "def parse_svs(filepath):\n",
    "    # Load the image\n",
    "    slide = OpenSlide(filepath)\n",
    "\n",
    "    # Calculate the level of the image pyramid to read from\n",
    "    target_size = (2400, 2400)\n",
    "    width, height = slide.dimensions\n",
    "    level = slide.get_best_level_for_downsample(max(width, height) / max(target_size))\n",
    "    level -= 1\n",
    "    if level < 0:\n",
    "        level = 0\n",
    "\n",
    "    level_width, level_height = slide.level_dimensions[level]\n",
    "\n",
    "    # Calculate the resize dimensions\n",
    "    target_size = (2400, 2400)\n",
    "    resize_width, resize_height = target_size\n",
    "\n",
    "    # Read the region at the desired level\n",
    "    region = slide.read_region((0, 0), level, (level_width, level_height))\n",
    "\n",
    "    # Resize the region to the target size\n",
    "    resized_region = region.resize((resize_width, resize_height), Image.BICUBIC)\n",
    "\n",
    "    region_rgb = resized_region.convert('RGB')\n",
    "    transform = Compose([\n",
    "    ToTensor(),  # Convert the image to a tensor\n",
    "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.25, 0.25, 0.25]),  # Normalize the image with mean and standard deviation\n",
    "    ])\n",
    "    tensor_img = transform(region_rgb)\n",
    "    return tensor_img\n",
    "\n",
    "\n",
    "\n",
    "def parse_clinical(filepath):\n",
    "    if filepath[-4:] == '.PDF':\n",
    "        # Convert the PDF to images\n",
    "        images = convert_from_path(f'{filepath}')\n",
    "\n",
    "        # Initialize an OCR engine (pytesseract)\n",
    "        pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'  # Update this path accordingly\n",
    "\n",
    "        # Initialize a list to hold all the OCRed text\n",
    "        text = []\n",
    "\n",
    "        # Perform OCR on each image\n",
    "        for i in range(len(images)):\n",
    "            text.append(pytesseract.image_to_string(images[i]))\n",
    "\n",
    "        # Concatenate all the text\n",
    "        all_text = ' '.join(text)\n",
    "\n",
    "    else:\n",
    "        with open(f'{filepath}', 'r') as file:\n",
    "            all_text = file.read()\n",
    "    # Tokenize the text\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer(all_text, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "    # Generate the embeddings\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # The last hidden state is the sequence of contextual embeddings. \n",
    "    embeddings = outputs.last_hidden_state[0, :, :].mean(0)\n",
    "    return embeddings\n",
    "\n",
    "def parse_copy_number_variation(filepath):\n",
    "    # Copy number Segments\n",
    "    df = pd.read_csv(f\"{filepath}\", sep='\\t')\n",
    "    df.loc[df.Chromosome == 'X', 'Chromosome'] = 23\n",
    "    df.loc[df.Chromosome == 'Y', 'Chromosome'] = 24\n",
    "    tensor = torch.from_numpy(df[['Chromosome', 'Start', 'End', 'Num_Probes', 'Segment_Mean']].astype(float).values)\n",
    "    return tensor\n",
    "\n",
    "def parse_gene_copy_number(filepath):\n",
    "    # Gene copy number\n",
    "    df = pd.read_csv(f\"{filepath}\", sep='\\t')\n",
    "    df['chromosome'] = df['chromosome'].str[3:]\n",
    "    df.loc[df['chromosome'] == 'X', 'chromosome'] = 23\n",
    "    df.loc[df['chromosome'] == 'Y', 'chromosome'] = 24\n",
    "    if len(df) != 60623:\n",
    "        print(\"ERROR\")\n",
    "    tensor = torch.from_numpy(df[['chromosome', 'start', 'end', 'copy_number', 'max_copy_number', 'min_copy_number']].astype(float).values)\n",
    "    return tensor\n",
    "\n",
    "def parse_allele_specific_copy_number(filepath):\n",
    "    df = pd.read_csv(f\"{filepath}\", sep='\\t')\n",
    "    df['Chromosome'] = df['Chromosome'].str[3:]\n",
    "    df.loc[df['Chromosome'] == 'X', 'Chromosome'] = 23\n",
    "    df.loc[df['Chromosome'] == 'Y', 'Chromosome'] = 24\n",
    "    tensor = torch.from_numpy(df[['Chromosome', 'Start', 'End', 'Copy_Number', 'Major_Copy_Number', 'Minor_Copy_Number']].astype(float).values)\n",
    "    return tensor\n",
    "\n",
    "def parse_proteomics(filepath):\n",
    "    filename = f'{filepath}'\n",
    "    df_proteome = pd.read_csv(filename, delimiter='\\t').merge(merged_proteomics, how='right', on='AGID')\n",
    "    tensor = torch.from_numpy(df_proteome['protein_expression'].values)\n",
    "    \n",
    "    return tensor # df_proteome <<< To merge\n",
    "\n",
    "class MultinomialDataset(Dataset):\n",
    "    def __init__(self, all_metadata):\n",
    "        self.patients = all_metadata['case_id'].unique()\n",
    "        self.metadata = all_metadata\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient = self.patients[idx]\n",
    "        met = self.metadata[self.metadata['case_id'] == patient].sort_values('updated_datetime', ascending=False).drop_duplicates('data_type')\n",
    "        \n",
    "        files = met['file_path']\n",
    "        \n",
    "        data = {}\n",
    "        \n",
    "        # X parse_gene_expression_quantification\n",
    "        # X parse_miRNA\n",
    "        # X parsed_isoform_to_miRNA\n",
    "        # X parse_svs\n",
    "        # X parse_maf\n",
    "        # X parse_clinical\n",
    "        # X parse_copy_number_variation\n",
    "        # X parse_gene_copy_number\n",
    "        # X parse_allele_specific_copy_number\n",
    "        # X parse_proteomics\n",
    "        \n",
    "        # TODO: Add methylation betas\n",
    "        data['svs'] = torch.full((3, 2400, 2400), float('nan'))\n",
    "        data['Masked Somatic Mutation'] = torch.full((1024, 135), float('nan')) # 437, 135\n",
    "        data['Gene Level Copy Number'] = torch.full((60623, 6), float('nan'))\n",
    "        data['Clinical'] = torch.full((768,), float('nan'))\n",
    "        data['Copy Number Segment'] = torch.full((1024, 5), float('nan'))\n",
    "        data['Protein Expression Quantification'] = torch.full((530,), float('nan'))\n",
    "        data['miRNA Expression Quantification'] = torch.full((1881, 2), float('nan'))\n",
    "        data['Isoform Expression Quantification'] = torch.full((1881, 2), float('nan'))\n",
    "        data['Allele-specific Copy Number Segment'] = torch.full((1024, 6), float('nan'))\n",
    "        data['Gene Expression Quantification'] = torch.full((60664, 6), float('nan'))\n",
    "        \n",
    "        for f in files:\n",
    "            try:\n",
    "                print(f)\n",
    "                metadata = self.metadata[self.metadata.file_path == f].iloc[0]\n",
    "                if f.endswith('.svs'):\n",
    "                    data['svs'] = parse_svs(f)\n",
    "                elif metadata.data_type in ['Masked Somatic Mutation', ]:\n",
    "                    temp_somatic = parse_maf(f)[:1024, :]\n",
    "                    data['Masked Somatic Mutation'][:temp_somatic.size()[0], :] = temp_somatic\n",
    "                elif metadata.data_type in ['Gene Level Copy Number', ]:\n",
    "                    data['Gene Level Copy Number'] = parse_gene_copy_number(f)\n",
    "                elif metadata.data_type in ['Clinical Supplement', 'Pathology Report']:\n",
    "                    data['Clinical'] = parse_clinical(f)\n",
    "                elif metadata.data_type in ['Copy Number Segment', 'Masked Copy Number Segment']:\n",
    "                    temp_copy = parse_copy_number_variation(f)\n",
    "                    data['Copy Number Segment'][:temp_copy.size()[0], :] = temp_copy\n",
    "                elif metadata.data_type in ['Protein Expression Quantification', ]:\n",
    "                    temp_proteom = parse_proteomics(f)\n",
    "                    data['Protein Expression Quantification'] = temp_proteom\n",
    "                elif metadata.data_type in ['miRNA Expression Quantification', ]:\n",
    "                    data['miRNA Expression Quantification'] = parse_miRNA(f)\n",
    "                elif metadata.data_type in ['Isoform Expression Quantification', ]:\n",
    "                    data['Isoform Expression Quantification'] = parsed_isoform_to_miRNA(f)\n",
    "                elif metadata.data_type in ['Allele-specific Copy Number Segment', ]:\n",
    "                    temp_allel = parse_allele_specific_copy_number(f)[:1024, :]\n",
    "                    data['Allele-specific Copy Number Segment'][:temp_allel.size()[0], :] = temp_allel\n",
    "                elif metadata.data_type in ['Gene Expression Quantification', ]:\n",
    "                    data['Gene Expression Quantification'] = parse_gene_expression_quantification(f)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        # Concatenate all the tensors together, fill in a constant value for missing files.\n",
    "#         data = torch.cat([d for k, d in data.items() if d is not None], dim=0)\n",
    "        return data\n",
    "\n",
    "# # Example usage:\n",
    "\n",
    "dataset = MultinomialDataset(processed_metadata)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "for batch in dataloader:\n",
    "    b = batch\n",
    "    break\n",
    "# dataset = MultiOmicsDataset(root_dir='/path/to/your/data')\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     # process your batch\n",
    "\n",
    "# filename = \"/mnt/Cancer_1/data/TCGA-EM-A3FO-01Z-00-DX1.84929069-A2FF-4588-98AF-77DE5D66F1C9.svs\"\n",
    "# a = parse_svs(filename).cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4850cd49",
   "metadata": {},
   "source": [
    "## Pre-Processing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201cc52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt(\"/mnt/Cancer_2/processed_data/patients.csv\", dataset.patients, delimiter=\",\", fmt='%s')\n",
    "for i in range(0, len(dataset)):\n",
    "    print(f\"{i} / {len(dataset)}\")\n",
    "    data = dataset[i]\n",
    "    for k, v in data.items():\n",
    "        torch.save(v, f\"/mnt/Cancer_2/processed_data/{i}_{'_'.join(k.split(' '))}.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e334dc",
   "metadata": {},
   "source": [
    "## Function to match swissprot to ensembl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_swissprot_id(ensembl_protein_id):\n",
    "    url = f\"https://www.uniprot.org/uploadlists/\"\n",
    "    params = {\n",
    "        \"from\": \"ENSEMBL_ID\",\n",
    "        \"to\": \"SWISSPROT\",\n",
    "        \"format\": \"tab\",\n",
    "        \"query\": ensembl_protein_id,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if not response.ok:\n",
    "        response.raise_for_status()\n",
    "        return None\n",
    "\n",
    "    lines = response.text.strip().split(\"\\n\")\n",
    "    if len(lines) < 2:\n",
    "        return None\n",
    "\n",
    "    # Parse the first line to get the column headers\n",
    "    headers = lines[0].split(\"\\t\")\n",
    "\n",
    "    # Parse the second line to get the data\n",
    "    data = lines[1].split(\"\\t\")\n",
    "\n",
    "    if len(data) < 2:\n",
    "        return None\n",
    "\n",
    "    # Assuming the Swiss-Prot ID is in the first column\n",
    "    swissprot_id_with_version = data[0]\n",
    "\n",
    "    return swissprot_id_with_version\n",
    "\n",
    "# Example usage\n",
    "ensembl_protein_id = \"ENSP00000366934.13\"\n",
    "swissprot_id = get_swissprot_id(ensembl_protein_id)\n",
    "print(swissprot_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
